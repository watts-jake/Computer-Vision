{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_mlp-mixer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4STWSFcqm6ja"
      },
      "source": [
        "# Biweekly Report\n",
        "\n",
        "# Jake Watts\n",
        "\n",
        "# Random Erasing + MLP-Mixer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xiNQo8DnDIY"
      },
      "source": [
        "Here I combine code for random erasing data processing with the MLP-Mixer architecture. I was interested in using random erasing with the MLP_Mixer because the random erasing paper compared performance when adding the model to CNN models so I wanted to see how adding it in to a different type of model would effect the performance. I first train a model with no random erasing, then one with random erasing and finally a thrid model with pixel-level random erasing to see how each performs.\n",
        "\n",
        "The code used for the MLP_Mixer comes from https://github.com/sayakpaul/MLP-Mixer-CIFAR10/blob/main/ResNet20.ipynb. I reworked the code so that it could work with the random eraser function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7wWS4PeUNT"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import io\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujLSNZMiedTD",
        "outputId": "37c97750-ecc8-41be-9f10-41bb58e1cbbe"
      },
      "source": [
        "try: \n",
        "    tpu = None\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: \n",
        "    strategy = tf.distribute.MirroredStrategy() \n",
        "\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of accelerators:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Ph2XcCnvNF"
      },
      "source": [
        "Defining model hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EPKismFei0F"
      },
      "source": [
        "RESIZE_TO = 72\n",
        "PATCH_SIZE = 9\n",
        "\n",
        "NUM_MIXER_LAYERS = 4\n",
        "HIDDEN_SIZE = 128\n",
        "MLP_SEQ_DIM = 64\n",
        "MLP_CHANNEL_DIM = 128\n",
        "\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 512 * strategy.num_replicas_in_sync"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI7oQ8S0eoIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1231f31b-acbb-4938-9c42-fb7735fa8d3e"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi2EKeAsoMXO"
      },
      "source": [
        "Function for data-augmentation and imaging resizing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiTacreGexBY"
      },
      "source": [
        "def get_augmentation_layers():\n",
        "    data_augmentation = keras.Sequential(\n",
        "        [\n",
        "            layers.experimental.preprocessing.Normalization(),\n",
        "            layers.experimental.preprocessing.Resizing(RESIZE_TO, RESIZE_TO),\n",
        "            layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "            layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
        "            layers.experimental.preprocessing.RandomZoom(\n",
        "                height_factor=0.2, width_factor=0.2\n",
        "            ),\n",
        "        ],\n",
        "        name=\"data_augmentation\",\n",
        "    )\n",
        "    # Compute the mean and the variance of the training data for normalization.\n",
        "    data_augmentation.layers[0].adapt(x_train)\n",
        "    \n",
        "    return data_augmentation"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icWJKeWVoTa3"
      },
      "source": [
        "Below is the code for the MLP-Mixer model which was based off the code from Appendix E of the MLP-Mixer paper (https://arxiv.org/pdf/2105.01601.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN88DUFreyJP"
      },
      "source": [
        "def mlp_block(x, mlp_dim):\n",
        "    x = layers.Dense(mlp_dim)(x)\n",
        "    x = tf.nn.gelu(x)\n",
        "    return layers.Dense(x.shape[-1])(x)\n",
        "\n",
        "def mixer_block(x, tokens_mlp_dim, channels_mlp_dim):\n",
        "    y = layers.LayerNormalization()(x)\n",
        "    y = layers.Permute((2, 1))(y)\n",
        "    \n",
        "    token_mixing = mlp_block(y, tokens_mlp_dim)\n",
        "    token_mixing = layers.Permute((2, 1))(token_mixing)\n",
        "    x = layers.Add()([x, token_mixing])\n",
        "    \n",
        "    y = layers.LayerNormalization()(x)\n",
        "    channel_mixing = mlp_block(y, channels_mlp_dim)\n",
        "    output = layers.Add()([x, channel_mixing])\n",
        "    return output\n",
        "\n",
        "def mlp_mixer(x, num_blocks, patch_size, hidden_dim, \n",
        "              tokens_mlp_dim, channels_mlp_dim,\n",
        "              num_classes=10):\n",
        "    x = layers.Conv2D(hidden_dim, kernel_size=patch_size,\n",
        "                      strides=patch_size, padding=\"valid\")(x)\n",
        "    x = layers.Reshape((x.shape[1]*x.shape[2], x.shape[3]))(x)\n",
        "\n",
        "    for _ in range(num_blocks):\n",
        "        x = mixer_block(x, tokens_mlp_dim, channels_mlp_dim)\n",
        "    \n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    return layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJT2gdC0o0DH"
      },
      "source": [
        "Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__g8IaqKe2bK"
      },
      "source": [
        "def create_mlp_mixer():\n",
        "    data_augmentation = get_augmentation_layers()\n",
        "    \n",
        "    inputs = layers.Input(shape=(32, 32, 3))\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = mlp_mixer(augmented, NUM_MIXER_LAYERS,\n",
        "                        PATCH_SIZE, HIDDEN_SIZE, \n",
        "                        MLP_SEQ_DIM, MLP_CHANNEL_DIM)\n",
        "    return tf.keras.Model(inputs, outputs, name=\"mlp_mixer\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HdjrtIWe7ld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28120b1f-6532-4af7-8801-f0193900f107"
      },
      "source": [
        "# We need to instantiate the model here for the callback below.\n",
        "with strategy.scope():\n",
        "    mlp_mixer_classifier = create_mlp_mixer()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7ff4f1bf8830>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZkEkoyBo5cY"
      },
      "source": [
        "Setting up model saving and progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xvXnJ_8e-t0"
      },
      "source": [
        "file_writer_cm = tf.summary.create_file_writer(os.path.join(os.getcwd(), 'saved_models'))\n",
        "\n",
        "def plot_to_image(figure):\n",
        "    # Save the plot to a PNG in memory.\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format=\"png\")\n",
        "    \n",
        "    # Closing the figure prevents it from being displayed directly inside\n",
        "    # the notebook.\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    \n",
        "    # Convert PNG buffer to TF image\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    \n",
        "    # Add the batch dimension\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    return image\n",
        "\n",
        "def log_progression(epoch, logs):\n",
        "    projections = mlp_mixer_classifier.layers[2].get_weights()[0]\n",
        "    p_min, p_max = projections.min(), projections.max()\n",
        "    projections = (projections - p_min) / (p_max - p_min)\n",
        "\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    n_filters, ix = 128, 1\n",
        "    for i in range(n_filters):\n",
        "        projection = projections[:, :, :, i]\n",
        "        plt.subplot(8, 16, ix)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(projection)\n",
        "        ix += 1\n",
        "\n",
        "    progress_image = plot_to_image(figure)\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Progression\", progress_image, step=epoch)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm1enZfzo-2o"
      },
      "source": [
        "Code to compile and train model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLHjjKkhfJPz"
      },
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001 * strategy.num_replicas_in_sync)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "    \n",
        "    progress_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_progression)\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback, progress_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, top_1_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(top_1_accuracy * 100, 2)}%\")\n",
        "    \n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci1_WA98pEeQ"
      },
      "source": [
        "After training the MLP-Mixer model for 25 epocjs the test accuracy is 69.47%. The accuracy is much lower than the ResNet model however this is not too much of a surprise as that model was much deeper and trained for longer. The MLP-Mixer also tends to have the highest performance for very large datasets. However I am more interested in seeing if the model performance changes with the addition of random erasing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaOzYZNjfKJv",
        "outputId": "53c2e6d4-714e-4d5b-bdbc-30b2f7b57818"
      },
      "source": [
        "with strategy.scope():\n",
        "    history, model = run_experiment(mlp_mixer_classifier)\n",
        "    model.save(f\"mlp_mixer_{NUM_MIXER_LAYERS}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "88/88 [==============================] - 26s 223ms/step - loss: 1.9026 - accuracy: 0.3064 - val_loss: 1.6054 - val_accuracy: 0.4138\n",
            "Epoch 2/25\n",
            "88/88 [==============================] - 18s 199ms/step - loss: 1.5305 - accuracy: 0.4481 - val_loss: 1.4184 - val_accuracy: 0.4818\n",
            "Epoch 3/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.3988 - accuracy: 0.4953 - val_loss: 1.3117 - val_accuracy: 0.5254\n",
            "Epoch 4/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.3129 - accuracy: 0.5280 - val_loss: 1.2421 - val_accuracy: 0.5450\n",
            "Epoch 5/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.2541 - accuracy: 0.5506 - val_loss: 1.2062 - val_accuracy: 0.5592\n",
            "Epoch 6/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.2029 - accuracy: 0.5734 - val_loss: 1.1619 - val_accuracy: 0.5820\n",
            "Epoch 7/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.1562 - accuracy: 0.5892 - val_loss: 1.1413 - val_accuracy: 0.5900\n",
            "Epoch 8/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.1221 - accuracy: 0.6010 - val_loss: 1.0906 - val_accuracy: 0.6056\n",
            "Epoch 9/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.0866 - accuracy: 0.6130 - val_loss: 1.0807 - val_accuracy: 0.6072\n",
            "Epoch 10/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.0563 - accuracy: 0.6232 - val_loss: 1.0229 - val_accuracy: 0.6384\n",
            "Epoch 11/25\n",
            "88/88 [==============================] - 18s 199ms/step - loss: 1.0268 - accuracy: 0.6354 - val_loss: 1.0110 - val_accuracy: 0.6410\n",
            "Epoch 12/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.0051 - accuracy: 0.6426 - val_loss: 1.0104 - val_accuracy: 0.6432\n",
            "Epoch 13/25\n",
            "88/88 [==============================] - 17s 196ms/step - loss: 0.9787 - accuracy: 0.6536 - val_loss: 0.9848 - val_accuracy: 0.6592\n",
            "Epoch 14/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 0.9503 - accuracy: 0.6608 - val_loss: 0.9660 - val_accuracy: 0.6600\n",
            "Epoch 15/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.9398 - accuracy: 0.6688 - val_loss: 0.9551 - val_accuracy: 0.6608\n",
            "Epoch 16/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.9180 - accuracy: 0.6759 - val_loss: 0.9630 - val_accuracy: 0.6584\n",
            "Epoch 17/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.8934 - accuracy: 0.6843 - val_loss: 0.9235 - val_accuracy: 0.6778\n",
            "Epoch 18/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.8822 - accuracy: 0.6884 - val_loss: 0.9175 - val_accuracy: 0.6810\n",
            "Epoch 19/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.8549 - accuracy: 0.6967 - val_loss: 0.8866 - val_accuracy: 0.6918\n",
            "Epoch 20/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 0.8408 - accuracy: 0.6997 - val_loss: 0.8870 - val_accuracy: 0.6928\n",
            "Epoch 21/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.8202 - accuracy: 0.7109 - val_loss: 0.9086 - val_accuracy: 0.6860\n",
            "Epoch 22/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.8135 - accuracy: 0.7138 - val_loss: 0.8785 - val_accuracy: 0.6898\n",
            "Epoch 23/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.7982 - accuracy: 0.7189 - val_loss: 0.8816 - val_accuracy: 0.6954\n",
            "Epoch 24/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 0.7759 - accuracy: 0.7256 - val_loss: 0.8522 - val_accuracy: 0.7034\n",
            "Epoch 25/25\n",
            "88/88 [==============================] - 17s 196ms/step - loss: 0.7643 - accuracy: 0.7301 - val_loss: 0.8574 - val_accuracy: 0.7006\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.8535 - accuracy: 0.6947\n",
            "Test accuracy: 69.47%\n",
            "INFO:tensorflow:Assets written to: mlp_mixer_4/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPoMxoHnptfH"
      },
      "source": [
        "Here I add the function for the random eraser and apply it to the data and retrain the data to see how it affects model performance. The rest of the model is unchanged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQU_cvDKpefU"
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        if input_img.ndim == 3:\n",
        "            img_h, img_w, img_c = input_img.shape\n",
        "        elif input_img.ndim == 2:\n",
        "            img_h, img_w = input_img.shape\n",
        "\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            if input_img.ndim == 3:\n",
        "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "            if input_img.ndim == 2:\n",
        "                c = np.random.uniform(v_l, v_h, (h, w))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IpifsCqx1gH"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "eraser = get_random_eraser(pixel_level = False)\n",
        "\n",
        "for i in range(len(x_train)):\n",
        "  eraser(x_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C-1h0IWzGRA"
      },
      "source": [
        "# We need to instantiate the model here for the callback below.\n",
        "with strategy.scope():\n",
        "    mlp_mixer_classifier = create_mlp_mixer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6byDf7Lyyy0"
      },
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001 * strategy.num_replicas_in_sync)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "    \n",
        "    progress_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_progression)\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback, progress_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, top_1_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(top_1_accuracy * 100, 2)}%\")\n",
        "    \n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BmWBepvqFTx"
      },
      "source": [
        "Surprisingly, the model's testing accuracy decreases from 69.47% to 67.92% after adding random erasing as I expected it to increase. It is possible I did not train for enough epochs for the model to adapt to the random erasing.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND33oJOT_TTB",
        "outputId": "0ce3e122-250a-41ce-cca2-fc6e397e1114"
      },
      "source": [
        "with strategy.scope():\n",
        "    history, model = run_experiment(mlp_mixer_classifier)\n",
        "    model.save(f\"mlp_mixer_{NUM_MIXER_LAYERS}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1/25\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "88/88 [==============================] - 39s 224ms/step - loss: 1.9422 - accuracy: 0.2874 - val_loss: 1.7149 - val_accuracy: 0.3822\n",
            "Epoch 2/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.6396 - accuracy: 0.4092 - val_loss: 1.5566 - val_accuracy: 0.4498\n",
            "Epoch 3/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.5172 - accuracy: 0.4506 - val_loss: 1.4418 - val_accuracy: 0.4850\n",
            "Epoch 4/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.4380 - accuracy: 0.4832 - val_loss: 1.3903 - val_accuracy: 0.5006\n",
            "Epoch 5/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.3940 - accuracy: 0.4984 - val_loss: 1.3534 - val_accuracy: 0.5112\n",
            "Epoch 6/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.3430 - accuracy: 0.5166 - val_loss: 1.3119 - val_accuracy: 0.5264\n",
            "Epoch 7/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.2940 - accuracy: 0.5356 - val_loss: 1.2604 - val_accuracy: 0.5480\n",
            "Epoch 8/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.2680 - accuracy: 0.5456 - val_loss: 1.2500 - val_accuracy: 0.5496\n",
            "Epoch 9/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.2332 - accuracy: 0.5590 - val_loss: 1.2008 - val_accuracy: 0.5684\n",
            "Epoch 10/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.2162 - accuracy: 0.5651 - val_loss: 1.1866 - val_accuracy: 0.5784\n",
            "Epoch 11/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.1711 - accuracy: 0.5836 - val_loss: 1.1644 - val_accuracy: 0.5774\n",
            "Epoch 12/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.1474 - accuracy: 0.5911 - val_loss: 1.1349 - val_accuracy: 0.5958\n",
            "Epoch 13/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.1219 - accuracy: 0.6018 - val_loss: 1.1442 - val_accuracy: 0.5938\n",
            "Epoch 14/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.1124 - accuracy: 0.6064 - val_loss: 1.1231 - val_accuracy: 0.5928\n",
            "Epoch 15/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.0878 - accuracy: 0.6118 - val_loss: 1.1078 - val_accuracy: 0.6022\n",
            "Epoch 16/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.0684 - accuracy: 0.6216 - val_loss: 1.1105 - val_accuracy: 0.6080\n",
            "Epoch 17/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 1.0534 - accuracy: 0.6279 - val_loss: 1.0676 - val_accuracy: 0.6216\n",
            "Epoch 18/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.0387 - accuracy: 0.6322 - val_loss: 1.0613 - val_accuracy: 0.6224\n",
            "Epoch 19/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 1.0080 - accuracy: 0.6425 - val_loss: 1.0797 - val_accuracy: 0.6212\n",
            "Epoch 20/25\n",
            "88/88 [==============================] - 18s 199ms/step - loss: 1.0006 - accuracy: 0.6456 - val_loss: 1.0415 - val_accuracy: 0.6330\n",
            "Epoch 21/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 0.9811 - accuracy: 0.6515 - val_loss: 1.0182 - val_accuracy: 0.6380\n",
            "Epoch 22/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 0.9708 - accuracy: 0.6564 - val_loss: 1.0034 - val_accuracy: 0.6390\n",
            "Epoch 23/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 0.9517 - accuracy: 0.6622 - val_loss: 1.0215 - val_accuracy: 0.6372\n",
            "Epoch 24/25\n",
            "88/88 [==============================] - 17s 197ms/step - loss: 0.9356 - accuracy: 0.6692 - val_loss: 1.0316 - val_accuracy: 0.6330\n",
            "Epoch 25/25\n",
            "88/88 [==============================] - 17s 198ms/step - loss: 0.9222 - accuracy: 0.6724 - val_loss: 1.0029 - val_accuracy: 0.6444\n",
            "313/313 [==============================] - 7s 21ms/step - loss: 0.9089 - accuracy: 0.6792\n",
            "Test accuracy: 67.92%\n",
            "INFO:tensorflow:Assets written to: mlp_mixer_4/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avZTQCyGqsjP"
      },
      "source": [
        "Finally I train the model for a third and final time using pixel_level random erasing which yield higher accuracy than regular random erasing with the ResNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjrPfG681EGO"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "pixel_level = True\n",
        "eraser = get_random_eraser(pixel_level = True)\n",
        "\n",
        "for i in range(len(x_train)):\n",
        "  eraser(x_train[i])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjVIC8a54NYX"
      },
      "source": [
        "# We need to instantiate the model here for the callback below.\n",
        "with strategy.scope():\n",
        "    mlp_mixer_classifier = create_mlp_mixer()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-34mqam4YBE"
      },
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001 * strategy.num_replicas_in_sync)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True ,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "    \n",
        "    progress_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_progression)\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback, progress_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, top_1_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(top_1_accuracy * 100, 2)}%\")\n",
        "    \n",
        "    return history, model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO5K1UhMq_dZ"
      },
      "source": [
        "With a testing accuracy of 68.4% this model performs better than regular random erasing which was expected but worse than no random erasing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGg_K5QL4a1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db65e402-c798-4750-d304-9067070dd3c9"
      },
      "source": [
        "with strategy.scope():\n",
        "    history, model = run_experiment(mlp_mixer_classifier)\n",
        "    model.save(f\"mlp_mixer_{NUM_MIXER_LAYERS}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1/25\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "88/88 [==============================] - 33s 113ms/step - loss: 1.9258 - accuracy: 0.2953 - val_loss: 1.6788 - val_accuracy: 0.3886\n",
            "Epoch 2/25\n",
            "88/88 [==============================] - 8s 90ms/step - loss: 1.6211 - accuracy: 0.4134 - val_loss: 1.5175 - val_accuracy: 0.4458\n",
            "Epoch 3/25\n",
            "88/88 [==============================] - 8s 90ms/step - loss: 1.4940 - accuracy: 0.4583 - val_loss: 1.4130 - val_accuracy: 0.4836\n",
            "Epoch 4/25\n",
            "88/88 [==============================] - 8s 91ms/step - loss: 1.4152 - accuracy: 0.4891 - val_loss: 1.3657 - val_accuracy: 0.5108\n",
            "Epoch 5/25\n",
            "88/88 [==============================] - 8s 90ms/step - loss: 1.3616 - accuracy: 0.5085 - val_loss: 1.3191 - val_accuracy: 0.5248\n",
            "Epoch 6/25\n",
            "88/88 [==============================] - 8s 90ms/step - loss: 1.3224 - accuracy: 0.5244 - val_loss: 1.2931 - val_accuracy: 0.5376\n",
            "Epoch 7/25\n",
            "88/88 [==============================] - 8s 91ms/step - loss: 1.2760 - accuracy: 0.5424 - val_loss: 1.2475 - val_accuracy: 0.5540\n",
            "Epoch 8/25\n",
            "88/88 [==============================] - 8s 91ms/step - loss: 1.2455 - accuracy: 0.5526 - val_loss: 1.1881 - val_accuracy: 0.5732\n",
            "Epoch 9/25\n",
            "88/88 [==============================] - 8s 91ms/step - loss: 1.2146 - accuracy: 0.5664 - val_loss: 1.1764 - val_accuracy: 0.5768\n",
            "Epoch 10/25\n",
            "88/88 [==============================] - 8s 93ms/step - loss: 1.1798 - accuracy: 0.5806 - val_loss: 1.1671 - val_accuracy: 0.5890\n",
            "Epoch 11/25\n",
            "88/88 [==============================] - 8s 91ms/step - loss: 1.1523 - accuracy: 0.5897 - val_loss: 1.1204 - val_accuracy: 0.6028\n",
            "Epoch 12/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 1.1240 - accuracy: 0.6007 - val_loss: 1.1401 - val_accuracy: 0.5984\n",
            "Epoch 13/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 1.1090 - accuracy: 0.6034 - val_loss: 1.0882 - val_accuracy: 0.6180\n",
            "Epoch 14/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 1.0804 - accuracy: 0.6153 - val_loss: 1.1004 - val_accuracy: 0.6144\n",
            "Epoch 15/25\n",
            "88/88 [==============================] - 8s 91ms/step - loss: 1.0613 - accuracy: 0.6212 - val_loss: 1.0543 - val_accuracy: 0.6258\n",
            "Epoch 16/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 1.0373 - accuracy: 0.6294 - val_loss: 1.0552 - val_accuracy: 0.6300\n",
            "Epoch 17/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 1.0232 - accuracy: 0.6366 - val_loss: 1.0330 - val_accuracy: 0.6396\n",
            "Epoch 18/25\n",
            "88/88 [==============================] - 8s 91ms/step - loss: 1.0049 - accuracy: 0.6449 - val_loss: 1.0376 - val_accuracy: 0.6350\n",
            "Epoch 19/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 0.9884 - accuracy: 0.6486 - val_loss: 1.0028 - val_accuracy: 0.6494\n",
            "Epoch 20/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 0.9685 - accuracy: 0.6593 - val_loss: 0.9856 - val_accuracy: 0.6494\n",
            "Epoch 21/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 0.9598 - accuracy: 0.6599 - val_loss: 0.9858 - val_accuracy: 0.6554\n",
            "Epoch 22/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 0.9371 - accuracy: 0.6669 - val_loss: 0.9738 - val_accuracy: 0.6560\n",
            "Epoch 23/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 0.9263 - accuracy: 0.6723 - val_loss: 0.9661 - val_accuracy: 0.6594\n",
            "Epoch 24/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 0.9084 - accuracy: 0.6759 - val_loss: 0.9719 - val_accuracy: 0.6598\n",
            "Epoch 25/25\n",
            "88/88 [==============================] - 8s 92ms/step - loss: 0.8913 - accuracy: 0.6853 - val_loss: 0.9578 - val_accuracy: 0.6678\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.9024 - accuracy: 0.6840\n",
            "Test accuracy: 68.4%\n",
            "INFO:tensorflow:Assets written to: mlp_mixer_4/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPLF8rQbrP4N"
      },
      "source": [
        "# Summary\n",
        "\n",
        "The addition of random erasing data augmentation to the MLP-Mixer did not increase the perfromance as I expected. This could be for several reasons. One could be I did not train the model for long enough. Another could be that the implementation of random erasing could be improved by adjusting the hyper-parameters or making changes to the model architecture. Or finally it could be that random erasing is more beneficial for convultional neural networks.\n",
        "\n",
        "Despite the results being different from anticipated, implementing this model gave me a much better understanding of MLP-Mixers and also adding in code from another source was definitely very challenging but helped me better understand the Tensorflow frameworks and python in general."
      ]
    }
  ]
}