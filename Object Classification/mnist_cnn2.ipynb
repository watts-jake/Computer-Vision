{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_cnn2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISXM_838C8Gl"
      },
      "source": [
        "\n",
        "\n",
        "# Biweekly Report\n",
        "\n",
        "# Jake Watts\n",
        "\n",
        "# Improving Model Performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swahvfPTJ1uL"
      },
      "source": [
        "Here I implement a much deeper model in order to improve model performance. In my first model I used two convolution layers. Whereas this model contains model 11 layers. In addition this model contains data standardization, data augmentation, validation data, image padding, dropout, batch normalization, and dense layers. In addition this model is created with tensorflow while the first model was created using pytorch, which gave me a better understanding of how the frameworks compare as I haven't used either before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQLsw-rXEgGX"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXSfsh-MJ2Hw"
      },
      "source": [
        "Loading in trading data and reshaping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkQYMizVB2K8"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hvo-02TLtA3"
      },
      "source": [
        "Creating validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGpqkBkn9s85"
      },
      "source": [
        "random_seed = 2\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = int(len(x_train)*0.05), random_state=random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yJV6mFNLvIO"
      },
      "source": [
        "Padding data to go from 28x28 images to 32x32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1e-z7Cgz6sv"
      },
      "source": [
        "x_train = np.pad(x_train, ((0,0), (2,2), (2,2), (0,0)), 'constant')\n",
        "x_val = np.pad(x_val, ((0,0), (2,2), (2,2), (0,0)), 'constant')\n",
        "x_test = np.pad(x_test, ((0,0), (2,2), (2,2), (0,0)), 'constant')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D0rA_hCL1bp"
      },
      "source": [
        "Standardizing data and adding une-hot encoding for the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlJcMudb5Bmx"
      },
      "source": [
        "mean_px = x_train.mean().astype(np.float32)\n",
        "std_px = x_train.std().astype(np.float32)\n",
        "x_train = (x_train - mean_px)/(std_px)\n",
        "\n",
        "mean_px = x_test.mean().astype(np.float32)\n",
        "std_px = x_test.std().astype(np.float32)\n",
        "x_test = (x_test - mean_px)/(std_px)\n",
        "\n",
        "mean_px = x_val.mean().astype(np.float32)\n",
        "std_px = x_val.std().astype(np.float32)\n",
        "x_val = (x_val - mean_px)/(std_px)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes = 10)\n",
        "y_val = to_categorical(y_val, num_classes = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvtkPxuhMCqG"
      },
      "source": [
        "Adding data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ1vPR_uGQTZ"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center = False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center = False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization = False,  # divide each input by its std\n",
        "        zca_whitening = False,  # apply ZCA whitening\n",
        "        rotation_range = 10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range = 0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range = 0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = False,  # randomly flip images\n",
        "        vertical_flip = False)  # randomly flip images\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFLsAdhsHw1o"
      },
      "source": [
        "Setting the learning rate to reduce val_loss stops improving over a period of two epochs. Will reduce by a factor of 0.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc34mO5C8SHY"
      },
      "source": [
        "variable_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9mPOIo4JfA0"
      },
      "source": [
        "The following is the most important part of the code for implementing the nueral network. The structure of this model was created by Jay Gupta for a kaggle competition and the model is called LeNet5v2. The architecture is as follows:\n",
        "\n",
        "$ConvNet \\rightarrow ConvNet \\rightarrow Pool \\rightarrow (Dropout) \\rightarrow ConvNet \\rightarrow Pool \\rightarrow (Dropout) \\rightarrow (Flatten) \\rightarrow FullyConnected \\rightarrow FullyConnected \\rightarrow Softmax$\n",
        "\n",
        "This network is different from the netowrk I implemented previously in several ways. Their are significantly more layers in this model which can hopefully pick up on more complex patterns. There are also four rather than two convulutional layers with 32 filters for the first two layers and 64 for the second two.\n",
        "\n",
        "Batch normalization is added for speed and stability while dropout is added to prevent overfitting. Max pooling with a stride of 2 is the same as the previous model and kernl-size in the convulutional layers is still 5 with a stride of 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvhlt7bF8zxQ"
      },
      "source": [
        "def LeNet5v2(input_shape = (32, 32, 1), classes = 10):\n",
        "    \"\"\"\n",
        "    Implementation of a modified LeNet-5.\n",
        "    Only those layers with learnable parameters are counted in the layer numbering.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential([\n",
        "        \n",
        "    # Layer 1\n",
        "    Conv2D(filters = 32, kernel_size = 5, strides = 1, activation = 'relu', input_shape = (32,32,1), kernel_regularizer=l2(0.0005), name = 'convolution_1'),\n",
        "    \n",
        "    # Layer 2\n",
        "    Conv2D(filters = 32, kernel_size = 5, strides = 1, name = 'convolution_2', use_bias=False),\n",
        "    \n",
        "    # Layer 3    \n",
        "    BatchNormalization(name = 'batchnorm_1'),\n",
        "        \n",
        "    # -------------------------------- #  \n",
        "    Activation(\"relu\"),\n",
        "    MaxPooling2D(pool_size = 2, strides = 2, name = 'max_pool_1'),\n",
        "    Dropout(0.25, name = 'dropout_1'),\n",
        "    # -------------------------------- #  \n",
        "        \n",
        "    # Layer 3\n",
        "    Conv2D(filters = 64, kernel_size = 3, strides = 1, activation = 'relu', kernel_regularizer=l2(0.0005), name = 'convolution_3'),\n",
        "        \n",
        "    # Layer 4\n",
        "    Conv2D(filters = 64, kernel_size = 3, strides = 1, name = 'convolution_4', use_bias=False),\n",
        "        \n",
        "    # Layer 5\n",
        "    BatchNormalization(name = 'batchnorm_2'),\n",
        "        \n",
        "    # -------------------------------- #  \n",
        "    Activation(\"relu\"),\n",
        "    MaxPooling2D(pool_size = 2, strides = 2, name = 'max_pool_2'),\n",
        "    Dropout(0.25, name = 'dropout_2'),\n",
        "    Flatten(name = 'flatten'),\n",
        "    # -------------------------------- #  \n",
        "        \n",
        "    # Layer 6\n",
        "    Dense(units = 256, name = 'fully_connected_1', use_bias=False),\n",
        "        \n",
        "    # Layer 7\n",
        "    BatchNormalization(name = 'batchnorm_3'),\n",
        "    \n",
        "    # -------------------------------- #  \n",
        "    Activation(\"relu\"),\n",
        "    # -------------------------------- #  \n",
        "        \n",
        "    # Layer 8\n",
        "    Dense(units = 128, name = 'fully_connected_2', use_bias=False),\n",
        "        \n",
        "    # Layer 9\n",
        "    BatchNormalization(name = 'batchnorm_4'),\n",
        "        \n",
        "    # -------------------------------- #  \n",
        "    Activation(\"relu\"),\n",
        "    # -------------------------------- #  \n",
        "        \n",
        "    # Layer 10\n",
        "    Dense(units = 84, name = 'fully_connected_3', use_bias=False),\n",
        "        \n",
        "    # Layer 11\n",
        "    BatchNormalization(name = 'batchnorm_5'),\n",
        "        \n",
        "    # -------------------------------- #  \n",
        "    Activation(\"relu\"),\n",
        "    Dropout(0.25, name = 'dropout_3'),\n",
        "    # -------------------------------- #  \n",
        "\n",
        "    # Output\n",
        "    Dense(units = 10, activation = 'softmax', name = 'output')\n",
        "        \n",
        "    ])\n",
        "    \n",
        "    model._name = 'LeNet5v2'\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6m4KCf5Mk1i"
      },
      "source": [
        "Specifying input shape and classes for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NNyNRYe80ym"
      },
      "source": [
        "LeNet5Model = LeNet5v2(input_shape = (32, 32, 1), classes = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LxbedktMrhi"
      },
      "source": [
        "Specifying the adam optimizer and cross entropy loss funxtion which were also used in the last model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r2RYaIH81VM"
      },
      "source": [
        "LeNet5Model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0-U6fYhJMkz"
      },
      "source": [
        "Print the model lets us see the details, shaoe abd number of parameters for each layer of the netowrk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ud1PZS_81bt",
        "outputId": "4bb5b5a3-6f82-43d5-87f0-c465f88a0d82"
      },
      "source": [
        "LeNet5Model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"LeNet5v2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "convolution_1 (Conv2D)       (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "convolution_2 (Conv2D)       (None, 24, 24, 32)        25600     \n",
            "_________________________________________________________________\n",
            "batchnorm_1 (BatchNormalizat (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pool_1 (MaxPooling2D)    (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "convolution_3 (Conv2D)       (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "convolution_4 (Conv2D)       (None, 8, 8, 64)          36864     \n",
            "_________________________________________________________________\n",
            "batchnorm_2 (BatchNormalizat (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pool_2 (MaxPooling2D)    (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "fully_connected_1 (Dense)    (None, 256)               262144    \n",
            "_________________________________________________________________\n",
            "batchnorm_3 (BatchNormalizat (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "fully_connected_2 (Dense)    (None, 128)               32768     \n",
            "_________________________________________________________________\n",
            "batchnorm_4 (BatchNormalizat (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "fully_connected_3 (Dense)    (None, 84)                10752     \n",
            "_________________________________________________________________\n",
            "batchnorm_5 (BatchNormalizat (None, 84)                336       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 84)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 84)                0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 390,562\n",
            "Trainable params: 389,434\n",
            "Non-trainable params: 1,128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aACmjdrVMN4d"
      },
      "source": [
        "Training data using validation data and training for 30 epochs, longer than the 10 epochs in the previous model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6OehAiN8VHS",
        "outputId": "1ea7cdb9-78b8-4de6-c194-db142978e0e8"
      },
      "source": [
        "history = LeNet5Model.fit(x_train, y_train, epochs = 30, batch_size = 64, callbacks = [variable_learning_rate], validation_data = (x_val,y_val))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "891/891 [==============================] - 13s 13ms/step - loss: 0.2273 - accuracy: 0.9404 - val_loss: 0.0706 - val_accuracy: 0.9850\n",
            "Epoch 2/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0829 - accuracy: 0.9809 - val_loss: 0.0664 - val_accuracy: 0.9813\n",
            "Epoch 3/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0675 - accuracy: 0.9840 - val_loss: 0.0618 - val_accuracy: 0.9857\n",
            "Epoch 4/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0559 - accuracy: 0.9869 - val_loss: 0.0448 - val_accuracy: 0.9910\n",
            "Epoch 5/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0508 - accuracy: 0.9874 - val_loss: 0.0543 - val_accuracy: 0.9883\n",
            "Epoch 6/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0461 - accuracy: 0.9896 - val_loss: 0.0411 - val_accuracy: 0.9893\n",
            "Epoch 7/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0425 - accuracy: 0.9896 - val_loss: 0.0536 - val_accuracy: 0.9897\n",
            "Epoch 8/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0404 - accuracy: 0.9903 - val_loss: 0.0386 - val_accuracy: 0.9910\n",
            "Epoch 9/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0371 - accuracy: 0.9911 - val_loss: 0.0373 - val_accuracy: 0.9897\n",
            "Epoch 10/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0360 - accuracy: 0.9916 - val_loss: 0.0577 - val_accuracy: 0.9880\n",
            "Epoch 11/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0327 - accuracy: 0.9924 - val_loss: 0.0384 - val_accuracy: 0.9917\n",
            "Epoch 12/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0236 - accuracy: 0.9950 - val_loss: 0.0259 - val_accuracy: 0.9953\n",
            "Epoch 13/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0187 - accuracy: 0.9961 - val_loss: 0.0273 - val_accuracy: 0.9950\n",
            "Epoch 14/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0172 - accuracy: 0.9964 - val_loss: 0.0283 - val_accuracy: 0.9943\n",
            "Epoch 15/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0148 - accuracy: 0.9971 - val_loss: 0.0249 - val_accuracy: 0.9953\n",
            "Epoch 16/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.0241 - val_accuracy: 0.9957\n",
            "Epoch 17/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0130 - accuracy: 0.9975 - val_loss: 0.0244 - val_accuracy: 0.9950\n",
            "Epoch 18/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0125 - accuracy: 0.9977 - val_loss: 0.0245 - val_accuracy: 0.9950\n",
            "Epoch 19/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0121 - accuracy: 0.9977 - val_loss: 0.0246 - val_accuracy: 0.9953\n",
            "Epoch 20/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.0245 - val_accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0125 - accuracy: 0.9977 - val_loss: 0.0245 - val_accuracy: 0.9957\n",
            "Epoch 22/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0125 - accuracy: 0.9975 - val_loss: 0.0244 - val_accuracy: 0.9957\n",
            "Epoch 23/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0124 - accuracy: 0.9977 - val_loss: 0.0241 - val_accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0126 - accuracy: 0.9975 - val_loss: 0.0245 - val_accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0120 - accuracy: 0.9979 - val_loss: 0.0242 - val_accuracy: 0.9957\n",
            "Epoch 26/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0122 - accuracy: 0.9978 - val_loss: 0.0244 - val_accuracy: 0.9960\n",
            "Epoch 27/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0120 - accuracy: 0.9978 - val_loss: 0.0243 - val_accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0117 - accuracy: 0.9978 - val_loss: 0.0242 - val_accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0122 - accuracy: 0.9978 - val_loss: 0.0242 - val_accuracy: 0.9957\n",
            "Epoch 30/30\n",
            "891/891 [==============================] - 11s 12ms/step - loss: 0.0121 - accuracy: 0.9978 - val_loss: 0.0247 - val_accuracy: 0.9960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE_7GY64G_PK"
      },
      "source": [
        "One-hot encoding y_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obiKNicTIDy2"
      },
      "source": [
        "y_test = to_categorical(y_test, num_classes = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h76txIM8HDUu"
      },
      "source": [
        "Testing the model to check accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq-75i2vGXa2",
        "outputId": "126f28e1-990b-41ed-9941-5b77cfa35d3f"
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = LeNet5Model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0183 - accuracy: 0.9960\n",
            "test loss, test acc: [0.018291659653186798, 0.9959999918937683]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuMli8Y6Mcsz"
      },
      "source": [
        "The testing accuracy is 99.6% which is an improvement over the previous much simpler model. The test accucracy is also equivalent to the training accuracy which indicates that adding in dropout to the network helped to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN8FEWCTMc5I"
      },
      "source": [
        "# Summary\n",
        "\n",
        "Overall it is not too big of a surprise that this model performed with an accuracy rate that was 1.6% higher than the previous model as there was a great number of features added in to improve the model. If you care about increasing accuracy above 98% and have more time to implement and train a better model than this model is definitely better. Having now worked with both pytorch and tensflow frameworks I think I prefer tensorflow slightly but I want to continue trying out both frameworks in future projects."
      ]
    }
  ]
}