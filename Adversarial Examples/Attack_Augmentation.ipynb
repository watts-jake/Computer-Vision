{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attack_Augmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBaYAOf5CvJa"
      },
      "source": [
        "# Biweekly Report 4\n",
        "\n",
        "# Jake Watts\n",
        "\n",
        "# Data Augmention + Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T63RvW7fitin"
      },
      "source": [
        "Since the trained model I used to evaluate the attack was trained without data augmentation, I decided to re-train it using data augmentation techniques. These techniques can generally improve model performance and prevent overfitting, however in this report I would like to test if these methods improve robustness against the Fast Sign Gradient Method.\n",
        "\n",
        "The data augmentation used in this report includes horizontal flipping, horizontal and vertical flipping and pixel erasing randomly applied to the training data for the Xception model. After training thte network I compare the accuracy on testing data with non perturbed images and then perturbed images with epsilon values of .01 and .001 to compare whether the effects are at all mitigated by training with augmented data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS3yEAM5Cjvz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Add\n",
        "from tensorflow.keras.layers import SeparableConv2D, ReLU, ELU\n",
        "from tensorflow.keras.layers import BatchNormalization, MaxPool2D\n",
        "from tensorflow.keras.layers import GlobalAvgPool2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import Model\n",
        "from keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh82evIPj4F-"
      },
      "source": [
        "Mounting Google Drive and loading data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwIjGNQUDxsP",
        "outputId": "539494b2-2d0e-4510-fe77-d5437e2de5d1"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coZqtVW5D2pI"
      },
      "source": [
        "def load_data():\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "  x_train = x_train.astype('float32') / 255\n",
        "  x_test = x_test.astype('float32') / 255\n",
        "  y_train = to_categorical(y_train, 10)\n",
        "  y_test = to_categorical(y_test, 10)\n",
        "  return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B--oX6YlBWF"
      },
      "source": [
        "Function for random erasing to be applied to the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-5-V2XZFBuN"
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        if input_img.ndim == 3:\n",
        "            img_h, img_w, img_c = input_img.shape\n",
        "        elif input_img.ndim == 2:\n",
        "            img_h, img_w = input_img.shape\n",
        "\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            if input_img.ndim == 3:\n",
        "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "            if input_img.ndim == 2:\n",
        "                c = np.random.uniform(v_l, v_h, (h, w))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkpBWGX2lJql"
      },
      "source": [
        "Loading training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPrFQOa5D5tf",
        "outputId": "27f06d37-03f5-48ea-c1bb-581e5d1e4153"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFLtHaI-lOQk"
      },
      "source": [
        "Functions for convolution and seperable convolutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLUxskxHD6n4"
      },
      "source": [
        "def conv_bn(x, filters, kernel_size, strides=1):\n",
        "    \n",
        "    x = Conv2D(filters=filters, \n",
        "               kernel_size = kernel_size, \n",
        "               strides=strides, \n",
        "               padding = 'same', \n",
        "               use_bias = False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    return x\n",
        "# creating separableConv-Batch Norm block\n",
        "\n",
        "def sep_bn(x, filters, kernel_size, strides=1):\n",
        "    \n",
        "    x = SeparableConv2D(filters=filters, \n",
        "                        kernel_size = kernel_size, \n",
        "                        strides=strides, \n",
        "                        padding = 'same', \n",
        "                        use_bias = False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHn4UztHlW-T"
      },
      "source": [
        "Defining the shortened version of the Xception architecture which performed the best in my previous biweekly report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDA0omFlEA7C"
      },
      "source": [
        "def entry_flow(x):\n",
        "    \n",
        "    x = conv_bn(x, filters =32, kernel_size =3, strides=2)\n",
        "    x = ELU()(x)\n",
        "    x = conv_bn(x, filters =64, kernel_size =3, strides=1)\n",
        "    tensor = ELU()(x)\n",
        "    \n",
        "    x = sep_bn(tensor, filters = 128, kernel_size =3)\n",
        "    x = ELU()(x)\n",
        "    x = sep_bn(x, filters = 128, kernel_size =3)\n",
        "    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)\n",
        "    \n",
        "    tensor = conv_bn(tensor, filters=128, kernel_size = 1,strides=2)\n",
        "    x = Add()([tensor,x])\n",
        "    \n",
        "    x = ELU()(x)\n",
        "    x = sep_bn(x, filters =256, kernel_size=3)\n",
        "    x = ELU()(x)\n",
        "    x = sep_bn(x, filters =256, kernel_size=3)\n",
        "    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)\n",
        "    \n",
        "    tensor = conv_bn(tensor, filters=256, kernel_size = 1,strides=2)\n",
        "    x = Add()([tensor,x])\n",
        "    \n",
        "    x = ELU()(x)\n",
        "    x = sep_bn(x, filters =728, kernel_size=3)\n",
        "    x = ELU()(x)\n",
        "    x = sep_bn(x, filters =728, kernel_size=3)\n",
        "    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)\n",
        "    \n",
        "    tensor = conv_bn(tensor, filters=728, kernel_size = 1,strides=2)\n",
        "    x = Add()([tensor,x])\n",
        "    return x\n",
        "# middle flow\n",
        "\n",
        "def middle_flow(tensor):\n",
        "    \n",
        "    for _ in range(8):\n",
        "        x = ELU()(tensor)\n",
        "        x = sep_bn(x, filters = 728, kernel_size = 3)\n",
        "        x = ELU()(x)\n",
        "        x = sep_bn(x, filters = 728, kernel_size = 3)\n",
        "        x = ELU()(x)\n",
        "        x = sep_bn(x, filters = 728, kernel_size = 3)\n",
        "        x = ELU()(x)\n",
        "        tensor = Add()([tensor,x])\n",
        "        \n",
        "        return tensor\n",
        "# exit flow\n",
        "\n",
        "def exit_flow(tensor):\n",
        "    x = GlobalAvgPool2D()(tensor)\n",
        "    \n",
        "    x = Dense (units = 10, activation = 'softmax')(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rpn-bXVlmom"
      },
      "source": [
        "Defining model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6yCqg5-EDxE"
      },
      "source": [
        "input = Input(shape = (32,32,3))\n",
        "x = entry_flow(input)\n",
        "x = middle_flow(x)\n",
        "output = exit_flow(x)\n",
        "\n",
        "model = Model (inputs=input, outputs=output)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_L05OXxloZr"
      },
      "source": [
        "Setting filepath to save model and defining callbacks options to save the best model, reduce the learning rate if the model is not imporving and end the training if the model is still not improving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ue4ljlWG1Fv"
      },
      "source": [
        "filepath = '/content/gdrive/My Drive/cifar10_xception_aug.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "es = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=10)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, es]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkb8BX_mGY0"
      },
      "source": [
        "Defining the image data generator to augment training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0riQ5-OFd5J"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False,  # randomly flip images\n",
        "    preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMruxWeJmJ7B"
      },
      "source": [
        "Compiling and training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhpCQWJ_FmfC",
        "outputId": "d935a698-3955-45e8-e0e9-783d66ecf8e4"
      },
      "source": [
        "datagen.fit(x_train)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "h_callback = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                       validation_data=(x_test, y_test),\n",
        "                       epochs=50,\n",
        "                       callbacks=callbacks)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 57s 25ms/step - loss: 1.5191 - accuracy: 0.4741 - val_loss: 1.4275 - val_accuracy: 0.5536\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.42753, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 1.1257 - accuracy: 0.6029 - val_loss: 1.4374 - val_accuracy: 0.5670\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.42753\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.9811 - accuracy: 0.6563 - val_loss: 0.9108 - val_accuracy: 0.6902\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.42753 to 0.91077, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8959 - accuracy: 0.6860 - val_loss: 0.8033 - val_accuracy: 0.7272\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.91077 to 0.80332, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.8284 - accuracy: 0.7107 - val_loss: 0.7522 - val_accuracy: 0.7454\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80332 to 0.75220, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.7742 - accuracy: 0.7293 - val_loss: 0.6897 - val_accuracy: 0.7676\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.75220 to 0.68965, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.7328 - accuracy: 0.7440 - val_loss: 0.6630 - val_accuracy: 0.7810\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.68965 to 0.66303, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6999 - accuracy: 0.7573 - val_loss: 0.6078 - val_accuracy: 0.7958\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.66303 to 0.60784, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6636 - accuracy: 0.7682 - val_loss: 0.6041 - val_accuracy: 0.7948\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.60784 to 0.60411, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.6419 - accuracy: 0.7775 - val_loss: 0.5885 - val_accuracy: 0.8025\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.60411 to 0.58853, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.6147 - accuracy: 0.7852 - val_loss: 0.5968 - val_accuracy: 0.8055\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.58853\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5898 - accuracy: 0.7950 - val_loss: 0.5427 - val_accuracy: 0.8182\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.58853 to 0.54266, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5692 - accuracy: 0.8020 - val_loss: 0.5507 - val_accuracy: 0.8105\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.54266\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5476 - accuracy: 0.8092 - val_loss: 0.5606 - val_accuracy: 0.8105\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.54266\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5324 - accuracy: 0.8140 - val_loss: 0.5943 - val_accuracy: 0.8117\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.54266\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5167 - accuracy: 0.8183 - val_loss: 0.5156 - val_accuracy: 0.8264\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.54266 to 0.51562, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5116 - accuracy: 0.8218 - val_loss: 0.5122 - val_accuracy: 0.8309\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.51562 to 0.51220, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4919 - accuracy: 0.8300 - val_loss: 0.5016 - val_accuracy: 0.8344\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.51220 to 0.50161, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4840 - accuracy: 0.8294 - val_loss: 0.4670 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.50161 to 0.46699, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4719 - accuracy: 0.8359 - val_loss: 0.5496 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.46699\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4580 - accuracy: 0.8401 - val_loss: 0.4641 - val_accuracy: 0.8464\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.46699 to 0.46410, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4479 - accuracy: 0.8435 - val_loss: 0.4690 - val_accuracy: 0.8456\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.46410\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4361 - accuracy: 0.8480 - val_loss: 0.4728 - val_accuracy: 0.8472\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.46410\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4270 - accuracy: 0.8504 - val_loss: 0.4568 - val_accuracy: 0.8483\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.46410 to 0.45680, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4204 - accuracy: 0.8524 - val_loss: 0.5400 - val_accuracy: 0.8269\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.45680\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.4137 - accuracy: 0.8552 - val_loss: 0.4459 - val_accuracy: 0.8545\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.45680 to 0.44588, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.4015 - accuracy: 0.8580 - val_loss: 0.4539 - val_accuracy: 0.8529\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.44588\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.3968 - accuracy: 0.8608 - val_loss: 0.4372 - val_accuracy: 0.8591\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.44588 to 0.43716, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.3867 - accuracy: 0.8654 - val_loss: 0.5109 - val_accuracy: 0.8418\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.43716\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.3833 - accuracy: 0.8650 - val_loss: 0.4728 - val_accuracy: 0.8439\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.43716\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.3743 - accuracy: 0.8688 - val_loss: 0.4542 - val_accuracy: 0.8539\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.43716\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.3664 - accuracy: 0.8704 - val_loss: 0.5029 - val_accuracy: 0.8440\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.43716\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.3611 - accuracy: 0.8721 - val_loss: 0.4729 - val_accuracy: 0.8528\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.43716\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2981 - accuracy: 0.8962 - val_loss: 0.3987 - val_accuracy: 0.8748\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.43716 to 0.39870, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2745 - accuracy: 0.9040 - val_loss: 0.3940 - val_accuracy: 0.8731\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.39870 to 0.39396, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2600 - accuracy: 0.9082 - val_loss: 0.4163 - val_accuracy: 0.8721\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.39396\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2555 - accuracy: 0.9109 - val_loss: 0.3994 - val_accuracy: 0.8767\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.39396\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2493 - accuracy: 0.9114 - val_loss: 0.3936 - val_accuracy: 0.8783\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.39396 to 0.39358, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2416 - accuracy: 0.9160 - val_loss: 0.3899 - val_accuracy: 0.8799\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.39358 to 0.38995, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2381 - accuracy: 0.9159 - val_loss: 0.4014 - val_accuracy: 0.8769\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.38995\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.2305 - accuracy: 0.9187 - val_loss: 0.4061 - val_accuracy: 0.8774\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.38995\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2333 - accuracy: 0.9184 - val_loss: 0.4000 - val_accuracy: 0.8791\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.38995\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2250 - accuracy: 0.9210 - val_loss: 0.4210 - val_accuracy: 0.8735\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.38995\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2167 - accuracy: 0.9242 - val_loss: 0.4066 - val_accuracy: 0.8788\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.38995\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2041 - accuracy: 0.9280 - val_loss: 0.3893 - val_accuracy: 0.8799\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.38995 to 0.38931, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.1980 - accuracy: 0.9314 - val_loss: 0.3975 - val_accuracy: 0.8801\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.38931\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.1981 - accuracy: 0.9305 - val_loss: 0.3898 - val_accuracy: 0.8801\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.38931\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.1944 - accuracy: 0.9326 - val_loss: 0.3855 - val_accuracy: 0.8835\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.38931 to 0.38550, saving model to /content/gdrive/My Drive/cifar10_xception_aug.h5\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.1862 - accuracy: 0.9360 - val_loss: 0.3924 - val_accuracy: 0.8828\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.38550\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.1887 - accuracy: 0.9351 - val_loss: 0.3923 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.38550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm_ECjOtrBJz"
      },
      "source": [
        "Adding data augmentation appears to help the problem I was had in my last biweekly report in which the training accuracy increased to 100% within the first 10 or so epochs while the validation accuracy tended to peak around 75-80%.\n",
        "\n",
        "In the graph below we can see that the validation accuracy tendes to stay closer to the validation accuracy with the largest gap between the two being about 5-6% as opposed to 25-30% in the last report. Adding data augmentation therfore appears to have been very helpful for model peformance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "O7bd-zEcmYCY",
        "outputId": "2e3a876f-e8d3-4ec8-af23-594d23edc874"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(h_callback.history['accuracy'])\n",
        "plt.plot(h_callback.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppCnB1RJsLw1"
      },
      "source": [
        "The accuracy of this model is 88% which is a large improvement over the 80% accuracy without any data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE7VdU2pnpx-",
        "outputId": "cc8ad6d3-e6ab-47f5-df7e-171e11056a41"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.3923 - accuracy: 0.8833\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3923356235027313, 0.8833000063896179]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AytWNEhCoBzH"
      },
      "source": [
        "Now we can see if the data aumentation helps the performance of the model against the Fast Sign Gradient Method. The code below is the function top create the adversial pattern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrIFztKbslAX"
      },
      "source": [
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "def create_adversarial_pattern(input_image, input_label):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(input_image)\n",
        "    prediction = model(input_image)\n",
        "    loss = loss_object(input_label, prediction)\n",
        "\n",
        "  # Get the gradients of the loss w.r.t to the input image.\n",
        "  gradient = tape.gradient(loss, input_image)\n",
        "  # Get the sign of the gradients to create the perturbation\n",
        "  signed_grad = tf.sign(gradient)\n",
        "  return signed_grad"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJw-SGCRs_lc"
      },
      "source": [
        "First I create the pertubred images using the first 1000 images in the training data (the same number used on the model without data augmentation) with an epsilon of 0.01. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUKmgR98mfzK"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "\n",
        "size = 1000\n",
        "x_test_attack = x_test[0:size]\n",
        "\n",
        "for i in range(size):\n",
        "\n",
        "  img = tf.convert_to_tensor(x_test[i:i+1], dtype=tf.float32)\n",
        "  label = tf.convert_to_tensor(y_test[i:i+1], dtype=tf.float32)\n",
        "  perturbations = create_adversarial_pattern(img, label)\n",
        "\n",
        "  epsilon = 0.01\n",
        "\n",
        "  x_test_attack[i] = img + epsilon*perturbations\n",
        "  x_test_attack[i] = tf.clip_by_value(x_test_attack[i], 0, 1)  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWlAOgjWtuFm"
      },
      "source": [
        "The accuracy on the model without data augmentation for this attack was 11.1% and the accuracy for this model is 11%. Therefore adding data augmentantion did not improve the model's performance when attacked. When using an epsilon of 0.01 both models are essentially useless."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toYC-2gqtnXS",
        "outputId": "bd38ee69-b23d-429f-f0ed-765bf96d5eb1"
      },
      "source": [
        "model.evaluate(x_test_attack, y_test[0:size]) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 7ms/step - loss: 7.5002 - accuracy: 0.1100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.500218391418457, 0.10999999940395355]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esVECIo3uJYQ"
      },
      "source": [
        "Now I change the epsilon value to 0.001 and evaluate the accuracy again. The model without data augmentation had an accuracy rate of 72.7% for this level of epsilon while this model with data augmentation has an accuracy rate of 79.7% meaning it performed better. This result makes sense as the model has a higher accuracy without perturbation so it would follow that the accuracy would be higher with slight perturbation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH4t0FQRo5Oy",
        "outputId": "aa9fade7-af73-4715-8ae3-628ac227e0f4"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "\n",
        "size = 1000\n",
        "x_test_attack = x_test[0:size]\n",
        "\n",
        "for i in range(size):\n",
        "\n",
        "  img = tf.convert_to_tensor(x_test[i:i+1], dtype=tf.float32)\n",
        "  label = tf.convert_to_tensor(y_test[i:i+1], dtype=tf.float32)\n",
        "  perturbations = create_adversarial_pattern(img, label)\n",
        "\n",
        "  epsilon = 0.001\n",
        "\n",
        "  x_test_attack[i] = img + epsilon*perturbations\n",
        "  x_test_attack[i] = tf.clip_by_value(x_test_attack[i], 0, 1) \n",
        "\n",
        "model.evaluate(x_test_attack, y_test[0:size]) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7030 - accuracy: 0.7970\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7030179500579834, 0.796999990940094]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrKekcdJox_W"
      },
      "source": [
        "# Summary\n",
        "\n",
        "While adding data augmentation appeared to increase the Xception model's performance without perturbation, it did not appear to improve the model's resistance to the attack. \n",
        "\n",
        "Since the Fast Sign Gradient Method changes pixels to maximize the loss function based of gradients after forward propogation, I guess it makes sense that simply adding data augmentation would not help much to defend against attacks. Adding in adversarial images created in a similar way would likely do more to defend agaisnt such attacks.\n",
        "\n",
        "Although adding data augmentation did not help defend against the attack, it was still useful as it greatly improved model performance and reduced the gap between training and validation accuracy."
      ]
    }
  ]
}