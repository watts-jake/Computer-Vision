# Biweekly Report 3
# Jake Watts

## Xception_Activation.ipynb
I implement and train the Xception network and compare it's performance using ReLU, ELU and no activation function on Cifar-10 data.

## Activation_Viz.ipynb
I visualize the activation function outputs from my trained Xception models comparing ReLU and ELU activations.

## Xception_Depth.ipynb
Here I experiment with changing the depth of the Xception model to see if it improves performance.
